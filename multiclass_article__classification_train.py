# -*- coding: utf-8 -*-
"""Multiclass_Article _Classification_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZF1K9UtfoTlKrmToE47DFXvNIx7F-6bh

IMPORT NECESSARY PACKAGES
"""

import os
import re
import json
import pickle
import datetime
import numpy as np
import pandas as pd

from tensorflow.keras.layers import Embedding
from tensorflow.keras.utils import plot_model
from tensorflow.keras import Input, Sequential
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import TensorBoard
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from multiclass_article_classification_module import ModelDevelopment
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional

"""# STEP 1) DATA LOADING"""

nlp_df = pd.read_csv('https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv')

"""# STEP 2) DATA INSPECTION"""

nlp_df.info()

nlp_df.describe().T

nlp_df.head()

"""CHECK DUPLICATES"""

nlp_df.duplicated().sum()

"""CHECK NANs"""

nlp_df.isna().sum()

"""# STEP 3) DATA CLEANING

REMOVE DUPLICATES
"""

nlp_df.duplicated().sum()

nlp_df.drop_duplicates(inplace=True)

"""REMOVE SYMBOLS, SPACES"""

category = nlp_df['category']
text = nlp_df['text']

# for index, texts in enumerate(text):
#   # to remove html tags
#   # anything within the <> will be removed including <>
#   # ? to tell re dont be greedy so it wont capture anything
#   # from the first < to the last > in the document
#   #category[index] = re.sub(text.split)
#   text[index] = re.sub('<.*?>','',texts)
#   text[index] = re.sub('[^a-zA-Z]',' ',texts).lower().split()
# text = nlp_df['text']

# for index, texts in enumerate(text):  #loop it to do it for all data
#     text[index] = re.sub('<.*?>','',texts)
#     text[index] = re.sub('[^a-zA-Z]',' ',texts).lower().split()

"""# STEP 4) FEATURE SELECTION

# STEP 5) DATA PREPROCESSING

X features
"""

vocab_size = 1000
oov_token = '<OOV>'
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)
tokenizer.fit_on_texts(text)

word_index = tokenizer.word_index
print(dict(list(word_index.items())))

text_int = tokenizer.texts_to_sequences(text)

max_len = np.median([len(text_int[i]) for i in range (len(text_int))])

padded_review = pad_sequences(text_int, 
                              maxlen = int(max_len),
                              padding = 'post',
                              truncating = 'post')

"""y target"""

ohe = OneHotEncoder(sparse=False)
category = ohe.fit_transform(np.expand_dims(category, axis=-1))

X_train, X_test, y_train, y_test = train_test_split(padded_review,
                                                    category,
                                                    test_size=0.3,
                                                    random_state=123)

"""# MODEL DEVELOPMENT"""

input_shape = np.shape(X_train)[1:]
nb_class = len(np.unique(y_train, axis=0))

md = ModelDevelopment()
nlp = md.sequential_model(input_shape, vocab_size = 1000, out_dims = 128, 
                          nb_class=5, nb_node=128, dropout_rate=0.3)

plot_model(nlp, show_shapes=(True))

nlp.compile(optimizer='adam',loss='categorical_crossentropy',
              metrics=['accuracy','acc'])

LOGS_PATH = os.path.join(os.getcwd(),'nlp_logs',datetime.datetime.now().
                         strftime('%Y%m%d-%H%M%S'))

tensorboard_callback = TensorBoard(log_dir=LOGS_PATH,histogram_freq=1)

hist = nlp.fit(X_train, y_train, epochs=10,
               validation_data=(X_test, y_test),
               callbacks=tensorboard_callback)

"""LOAD TENSORBOARD"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir nlp_logs

"""To show available keys:"""

hist.history.keys()

"""# MODEL ANALYSIS"""

y_pred = np.argmax(nlp.predict(X_test), axis=1)
y_actual = np.argmax(y_test, axis=1)

print(classification_report(y_actual, y_pred))

"""# MODEL SAVING

*tokenizer*
"""

TOKENIZER_SAVE_PATH = os.path.join(os.getcwd(), 'tokenizer.json')

token_json = tokenizer.to_json()
with open(TOKENIZER_SAVE_PATH, 'w') as file:
  json.dump(token_json, file)

"""*onehotencoder*"""

OHE_SAVE_PATH = os.path.join(os.getcwd(), 'ohe.pkl')

with open(OHE_SAVE_PATH, 'wb') as file:
  pickle.dump(ohe, file)

"""*model*"""

NLP_SAVE_PATH = os.path.join(os.getcwd(), 'nlp.h5')
nlp.save(NLP_SAVE_PATH)